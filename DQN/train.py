"""
ìµœì í™” í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
í™•ì¥ëœ í•™ìŠµ ì‹œê°„ì„ í™œìš©í•œ ì‹¬í™” ê°•í™”í•™ìŠµ
"""

import torch
import numpy as np
import time
import argparse
import signal
import sys
import os
from typing import Dict, List
import warnings
warnings.filterwarnings('ignore')

from curriculum import (
    CurriculumConfig,
    print_curriculum_progress,
    CURRICULUM_MILESTONES,
    get_stage_recommendations
)
from environment import AirSimDroneEnv
from dqn_agent import DQNAgent
from utils import (
    Logger, PerformanceMonitor, Visualizer, MemoryManager,
    ConfigValidator, ModelCheckpointer, set_random_seeds,
    create_experiment_directory, format_time, calculate_eta
)
from real_time_monitor import RealTimeMonitor, EpisodeState
from performance_analyzer import PerformanceAnalyzer

class ExtendedTrainingManager:
    """ìµœì í™” í•™ìŠµ ê´€ë¦¬ì"""

    def __init__(self, experiment_name: str = None, resume_from: str = None):
        self.config = CurriculumConfig()
        self.experiment_name = experiment_name or f"extended_training_{int(time.time())}"
        self.resume_from = resume_from

        # ì‹¤í—˜ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.experiment_dir = create_experiment_directory("experiments", self.experiment_name)
        self.config.SAVE_DIR = os.path.join(self.experiment_dir, "checkpoints")
        self.config.LOG_DIR = os.path.join(self.experiment_dir, "logs")

        # ëœë¤ ì‹œë“œ ì„¤ì •
        set_random_seeds(self.config.RANDOM_SEED)

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.env = None
        self.agent = None
        self.logger = None
        self.monitor = None
        self.visualizer = None
        self.checkpointer = None
        self.performance_analyzer = PerformanceAnalyzer()

        # í•™ìŠµ ìƒíƒœ
        self.start_episode = 0
        self.current_stage = 'basic_stabilization'
        self.stage_best_rewards = {}
        self.milestone_achieved = set()

        # í™•ì¥ í•™ìŠµ í†µê³„
        self.recent_rewards = []
        self.stage_performance = {}
        self.learning_acceleration = 0
        self.convergence_tracker = []

        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°
        self.real_time_monitor = RealTimeMonitor()
        self.use_real_time_display = True

        # í•™ìŠµ ì¤‘ë‹¨ ì²˜ë¦¬
        self.training_interrupted = False
        signal.signal(signal.SIGINT, self._signal_handler)

        print(f"Extended Training: {self.experiment_name}")
        print(f"ğŸ“ ì‹¤í—˜ ë””ë ‰í† ë¦¬: {self.experiment_dir}")

    def _signal_handler(self, signum, frame):
        print("\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ ì‹ í˜¸ ìˆ˜ì‹ ... ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ì¤‘...")
        self.training_interrupted = True
        if hasattr(self, 'real_time_monitor'):
            self.real_time_monitor.cleanup_terminal()

    def initialize_components(self):
        """ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”"""
        print("ğŸ”§ í™•ì¥ í•™ìŠµ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”...")

        # ìµœì‹  ê¸°ë²• ëª¨ë‹ˆí„°ë§ ë³€ìˆ˜ë“¤
        self.categorical_metrics = []
        self.curiosity_metrics = []
        self.ucb_metrics = []

        # í™˜ê²½ ì´ˆê¸°í™”
        try:
            self.env = AirSimDroneEnv()
            print("âœ… AirSim í™˜ê²½ ì—°ê²° ì„±ê³µ")
        except Exception as e:
            print(f"âŒ AirSim í™˜ê²½ ì—°ê²° ì‹¤íŒ¨: {e}")
            return False

        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self.agent = DQNAgent(self.config)
        print("âœ… DQN ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        # í•™ìŠµ ì¬ì‹œì‘
        if self.resume_from:
            self.start_episode = self.agent.load_model(self.resume_from)
            # ëª¨ë¸ ë¡œë“œ ì™„ë£Œ

        # ë¡œê±° ì´ˆê¸°í™”
        self.logger = Logger(self.config.LOG_DIR, self.experiment_name)
        self.logger.log_model_info(self.agent.q_network)

        # ê¸°íƒ€ ì»´í¬ë„ŒíŠ¸
        self.monitor = PerformanceMonitor()
        self.visualizer = Visualizer(os.path.join(self.experiment_dir, "plots"))
        self.checkpointer = ModelCheckpointer(self.config.SAVE_DIR, max_checkpoints=10)  # ë” ë§ì€ ì²´í¬í¬ì¸íŠ¸

        print("âœ… ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
        return True

    def update_stage_configuration(self, episode: int):
        """í™•ì¥ í•™ìŠµì„ ìœ„í•œ ë™ì  ì„¤ì • ì—…ë°ì´íŠ¸"""
        # ì»¤ë¦¬í˜ëŸ¼ ì„¤ì • ì—…ë°ì´íŠ¸
        stage_name, stage_config = self.config.update_config_for_episode(episode)

        # í™˜ê²½ ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸
        if hasattr(self.env, 'config'):
            self.env.config.MAX_EPISODE_STEPS = stage_config['max_episode_steps']
            self.env.config.TARGET_RADIUS = stage_config['target_radius']
            self.env.config.REWARDS.update(stage_config['rewards'])

        # ì—ì´ì „íŠ¸ ì„¤ì • ë™ì  ì—…ë°ì´íŠ¸
        new_lr = self.config.get_learning_rate_schedule(episode)
        for param_group in self.agent.optimizer.param_groups:
            param_group['lr'] = new_lr

        # íƒ€ê²Ÿ ì—…ë°ì´íŠ¸ ì£¼ê¸° ë™ì  ì¡°ì •
        new_target_freq = self.config.get_target_update_frequency(episode)
        self.config.TARGET_UPDATE_FREQUENCY = new_target_freq

        # ìŠ¤í…Œì´ì§€ ë³€ê²½ ì‹œ ìµœì í™” ì ìš©
        if stage_name != self.current_stage:
            print(f"\nğŸ¯ ìŠ¤í…Œì´ì§€ ë³€ê²½: {self.current_stage} â†’ {stage_name}")
            print(f"ğŸ“‹ ìƒˆë¡œìš´ í¬ì»¤ìŠ¤: {stage_config['focus']}")
            print(f"ğŸ”„ ìƒˆë¡œìš´ íƒ€ê²Ÿ ì—…ë°ì´íŠ¸ ì£¼ê¸°: {new_target_freq}")

            # ë‹¨ê³„ë³„ ìµœì í™”
            self.optimize_for_stage(stage_name, episode)
            self.current_stage = stage_name

        return stage_name, stage_config

    def optimize_for_stage(self, stage_name: str, episode: int):
        """ê° ìŠ¤í…Œì´ì§€ë³„ íŠ¹í™” ìµœì í™”"""
        if stage_name == 'basic_stabilization':
            # ê¸°ë³¸ ì•ˆì •í™”: ì¶©ëŒ íšŒí”¼ ê°•í™”
            self.agent.memory.beta = min(1.0, self.agent.memory.beta + 0.05)
        elif stage_name == 'basic_target_detection':
            # ê¸°ë³¸ ëª©í‘œ íƒì§€: íƒí—˜-í™œìš© ê· í˜• ì¡°ì •
            if hasattr(self.agent, 'epsilon'):
                self.agent.epsilon = max(self.agent.epsilon * 0.95, 0.3)
        elif stage_name == 'precision_navigation':
            # ì •ë°€ ë‚´ë¹„ê²Œì´ì…˜: ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¦ê°€
            self.config.TARGET_UPDATE_FREQUENCY = max(150, self.config.TARGET_UPDATE_FREQUENCY - 50)
        elif stage_name == 'multi_target_handling':
            # ë‹¤ì¤‘ ëª©í‘œ: ë°°ì¹˜ í¬ê¸° ìµœì í™”
            if episode > 600 and self.config.BATCH_SIZE < 64:
                self.config.BATCH_SIZE = min(64, self.config.BATCH_SIZE + 16)
        elif stage_name == 'master_optimization':
            # ë§ˆìŠ¤í„° ìµœì í™”: ë¯¸ì„¸ ì¡°ì •
            self.agent.memory.beta = 1.0  # ìµœëŒ€ ìš°ì„ ìˆœìœ„ ì ìš©

        print(f"âš¡ {stage_name} ë‹¨ê³„ìš© íŠ¹í™” ìµœì í™” ì ìš©")

    def run_episode(self, display_episode: int) -> Dict:
        """í™•ì¥ ìµœì í™”ëœ ì—í”¼ì†Œë“œ ì‹¤í–‰"""
        episode = display_episode - 1

        # ìŠ¤í…Œì´ì§€ ì„¤ì • ì—…ë°ì´íŠ¸
        stage_name, stage_config = self.update_stage_configuration(episode)

        # Epsilon ì—…ë°ì´íŠ¸
        self.agent.epsilon = self.config.get_epsilon_schedule(episode)

        # ì—í”¼ì†Œë“œ ì‹¤í–‰
        state = self.env.reset()
        episode_reward = 0
        episode_length = 0
        episode_loss = 0
        loss_count = 0
        targets_found = 0
        efficiency_score = 0
        cumulative_reward = 0
        q_values_history = []

        start_time = time.time()

        while not self.training_interrupted:
            # í˜„ì¬ ìƒíƒœ ì •ë³´
            current_pos = self.env.get_current_position()
            target_pos = self.env.config.TARGET_POSITIONS[self.env.current_target_idx]
            distance_to_target = self.env._calculate_distance_to_target()

            # í–‰ë™ ì„ íƒ ë° Qê°’ ì¶”ì  (íƒ€ì´ë° ì œì–´ ì¶”ê°€)
            action_start_time = time.time()
            action = self.agent.select_action(state, training=True)

            # í–‰ë™ ì„ íƒ í›„ ìµœì†Œ ëŒ€ê¸°ì‹œê°„ ë³´ì¥ (ì•ˆì •ì ì¸ í•™ìŠµì„ ìœ„í•¨)
            elapsed_action_time = time.time() - action_start_time
            if elapsed_action_time < self.config.MIN_ACTION_INTERVAL:
                time.sleep(self.config.MIN_ACTION_INTERVAL - elapsed_action_time)

            # Qê°’ ëª¨ë‹ˆí„°ë§ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ê°œì„ )
            if stage_name == 'master_optimization' and episode_length % 10 == 0:  # ë§¤ 10ìŠ¤í…ë§ˆë‹¤ë§Œ
                with torch.no_grad():
                    state_tensor = self.agent._dict_to_tensor(state)
                    q_values = self.agent.q_network(state_tensor)
                    q_values_history.append(q_values.mean().item())
                    # í…ì„œ ì¦‰ì‹œ ì‚­ì œ
                    del state_tensor, q_values

            # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„° ì—…ë°ì´íŠ¸
            if self.use_real_time_display and episode_length % 1 == 0:
                monitor_state = EpisodeState(
                    episode=display_episode,
                    step=episode_length,
                    max_steps=stage_config['max_episode_steps'],
                    reward=episode_reward / max(episode_length, 1),
                    cumulative_reward=cumulative_reward,
                    current_pos=current_pos,
                    target_pos=target_pos,
                    distance_to_target=distance_to_target,
                    targets_found=targets_found,
                    total_targets=len(self.env.config.TARGET_POSITIONS),
                    epsilon=self.agent.epsilon,
                    learning_rate=self.config.get_learning_rate_schedule(episode),
                    stage=stage_name,
                    action=str(action),
                    collision=False,
                    reason="in_progress"
                )
                self.real_time_monitor.step_update(monitor_state)

            # í™˜ê²½ ìŠ¤í…
            next_state, reward, done, info = self.env.step(action)

            # ê³ ê¸‰ ë³´ìƒ ì¶”ì 
            targets_this_episode = info.get('targets_found', 0)
            if targets_this_episode > targets_found:
                efficiency_score += (targets_this_episode - targets_found) * 10

            # ê²½í—˜ ì €ì¥
            self.agent.store_experience(state, action, reward, next_state, done)

            # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if len(self.agent.memory) >= self.config.MIN_REPLAY_SIZE:
                # ë‹¨ê³„ë³„ ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¡°ì • (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤)
                update_frequency = {
                    'basic_stabilization': 8,       # ë§¤ 8ìŠ¤í…ë§ˆë‹¤ (ë©”ëª¨ë¦¬ ì ˆì•½)
                    'basic_target_detection': 6,    # ë§¤ 6ìŠ¤í…ë§ˆë‹¤
                    'precision_navigation': 4,      # ë§¤ 4ìŠ¤í…ë§ˆë‹¤
                    'multi_target_handling': 3,     # ë§¤ 3ìŠ¤í…ë§ˆë‹¤
                    'master_optimization': 2        # ë§¤ 2ìŠ¤í…ë§ˆë‹¤
                }.get(stage_name, 4)

                if episode_length % update_frequency == 0:
                    loss = self.agent.update_model()
                    if loss > 0:
                        episode_loss += loss
                        loss_count += 1

                    # ë©”ëª¨ë¦¬ ì •ë¦¬ (ë§¤ ì—…ë°ì´íŠ¸ë§ˆë‹¤)
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

            # ìƒíƒœ ì—…ë°ì´íŠ¸
            state = next_state
            episode_reward += reward
            cumulative_reward += reward
            episode_length += 1
            targets_found = info.get('targets_found', 0)

            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´
            if done or episode_length >= stage_config['max_episode_steps']:
                break

        # ìˆ˜ë ´ ì¶”ì 
        self.convergence_tracker.append(cumulative_reward)
        if len(self.convergence_tracker) > 50:
            self.convergence_tracker.pop(0)

        # í™˜ê²½ ë Œë”ë§ ì •ë³´
        render_info = self.env.render()

        # ìµœì¢… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„° ì—…ë°ì´íŠ¸
        if self.use_real_time_display:
            final_monitor_state = EpisodeState(
                episode=display_episode,
                step=episode_length,
                max_steps=stage_config['max_episode_steps'],
                reward=episode_reward / max(episode_length, 1),
                cumulative_reward=cumulative_reward,
                current_pos=current_pos,
                target_pos=target_pos,
                distance_to_target=render_info.get('distance_to_target', 0),
                targets_found=targets_found,
                total_targets=len(self.env.config.TARGET_POSITIONS),
                epsilon=self.agent.epsilon,
                learning_rate=self.config.get_learning_rate_schedule(episode),
                stage=stage_name,
                action=str(action),
                collision=info.get('reason') == 'collision',
                reason=info.get('reason', 'unknown')
            )
            self.real_time_monitor.episode_complete(final_monitor_state)

        # ì„±ëŠ¥ ì¶”ì  ì—…ë°ì´íŠ¸
        self.update_performance_tracking(cumulative_reward, stage_name)

        # ë°˜í™˜ ì •ë³´ ì¶”ì¶œ (render_info ì‚­ì œ ì „ì—)
        exploration_coverage = render_info.get('exploration_coverage', 0)
        distance_to_target_final = render_info.get('distance_to_target', 0)

        # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        # ë¡œì»¬ ë³€ìˆ˜ ì •ë¦¬ (í° ê°ì²´ë“¤)
        del state, render_info
        if q_values_history:
            del q_values_history

        return {
            'reward': cumulative_reward,
            'length': episode_length,
            'loss': episode_loss / max(loss_count, 1),
            'targets_found': targets_found,
            'epsilon': self.agent.epsilon,
            'time': time.time() - start_time,
            'exploration_coverage': exploration_coverage,
            'distance_to_target': distance_to_target_final,
            'reason': info.get('reason', 'unknown'),
            'stage': stage_name,
            'stage_focus': stage_config['focus'],
            'efficiency_score': efficiency_score,
            'learning_rate': self.config.get_learning_rate_schedule(episode),
            'avg_q_value': np.mean(q_values_history) if q_values_history else 0,
            'convergence_trend': np.mean(self.convergence_tracker[-10:]) if len(self.convergence_tracker) >= 10 else 0
        }

    def update_performance_tracking(self, reward: float, stage_name: str):
        """ì„±ëŠ¥ ì¶”ì  ë° ìˆ˜ë ´ ë¶„ì„"""
        self.recent_rewards.append(reward)
        if len(self.recent_rewards) > 100:  # ìµœê·¼ 100 ì—í”¼ì†Œë“œ
            self.recent_rewards.pop(0)

        # ìŠ¤í…Œì´ì§€ë³„ ì„±ëŠ¥ ê¸°ë¡
        if stage_name not in self.stage_performance:
            self.stage_performance[stage_name] = []
        self.stage_performance[stage_name].append(reward)

        # í•™ìŠµ ê°€ì†ë„ ê³„ì‚°
        if len(self.recent_rewards) >= 40:
            recent_avg = np.mean(self.recent_rewards[-20:])
            older_avg = np.mean(self.recent_rewards[-40:-20])
            self.learning_acceleration = recent_avg - older_avg

    def check_milestones(self, episode: int, stats: Dict):
        """í™•ì¥ ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„± í™•ì¸"""
        if episode in CURRICULUM_MILESTONES and episode not in self.milestone_achieved:
            milestone = CURRICULUM_MILESTONES[episode]
            expected_reward = milestone['expected_reward']

            # ìµœê·¼ ì„±ëŠ¥ í‰ê°€ (ë” ì—„ê²©í•œ ê¸°ì¤€)
            recent_avg = np.mean(self.recent_rewards) if self.recent_rewards else 0
            achievement_ratio = recent_avg / expected_reward if expected_reward > 0 else 1.0

            # ë§ˆì¼ìŠ¤í†¤ ì²´í¬
            print(f"ğŸ“Š ìµœê·¼ í‰ê·  ë³´ìƒ: {recent_avg:.2f} (ëª©í‘œ: {expected_reward})")
            print(f"ğŸ¯ ë‹¬ì„±ë„: {achievement_ratio:.1%}")
            print(f"ğŸ“ˆ í•™ìŠµ ê°€ì†ë„: {self.learning_acceleration:.2f}")

            # ë‹¬ì„± ê¸°ì¤€ (ë” ì—„ê²©)
            if achievement_ratio >= 0.8 or recent_avg > expected_reward * 0.8:
                print("âœ… ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„±!")
                self.milestone_achieved.add(episode)
                self.apply_milestone_bonus(episode)
            else:
                print("âš ï¸ ë§ˆì¼ìŠ¤í†¤ ë¯¸ë‹¬ì„± - í•™ìŠµ ê°•í™” ì ìš©")
                self.apply_learning_boost(episode)

    def apply_milestone_bonus(self, episode: int):
        """ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„± ì‹œ ìµœì í™” ë³´ë„ˆìŠ¤"""
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì¦‰ì‹œ ì—…ë°ì´íŠ¸
        self.agent.target_network.load_state_dict(self.agent.q_network.state_dict())

        # ë©”ëª¨ë¦¬ ìµœì í™”
        MemoryManager.optimize_memory()

        # ìŠ¤ì¼€ì¤„ëŸ¬ ìŠ¤í… (ì„±ëŠ¥ ê¸°ë°˜)
        recent_avg = np.mean(self.recent_rewards[-20:]) if len(self.recent_rewards) >= 20 else 0
        self.agent.scheduler.step(recent_avg)

        print("ğŸ‰ ë§ˆì¼ìŠ¤í†¤ ë³´ë„ˆìŠ¤: íƒ€ê²Ÿ ì—…ë°ì´íŠ¸ + ë©”ëª¨ë¦¬ ìµœì í™” + ìŠ¤ì¼€ì¤„ëŸ¬ ì¡°ì •")

    def apply_learning_boost(self, episode: int):
        """í•™ìŠµ ë¶€ì§„ ì‹œ ì ì‘ì  ë¶€ìŠ¤íŠ¸"""
        # í˜„ì¬ ë‹¨ê³„ì— ë”°ë¥¸ ë¶€ìŠ¤íŠ¸ ì „ëµ
        stage_name = self.current_stage

        if stage_name in ['basic_stabilization', 'basic_target_detection']:
            # ì´ˆê¸° ë‹¨ê³„: í•™ìŠµë¥  ë¶€ìŠ¤íŠ¸
            current_lr = self.agent.optimizer.param_groups[0]['lr']
            boost_lr = min(0.002, current_lr * 1.3)
            for param_group in self.agent.optimizer.param_groups:
                param_group['lr'] = boost_lr
            print(f"âš¡ ì´ˆê¸° ë‹¨ê³„ ë¶€ìŠ¤íŠ¸: í•™ìŠµë¥  {current_lr:.5f} â†’ {boost_lr:.5f}")

        elif stage_name in ['precision_navigation', 'multi_target_handling']:
            # ì¤‘ê°„ ë‹¨ê³„: íƒ€ê²Ÿ ì—…ë°ì´íŠ¸ ë¹ˆë„ ì¦ê°€
            self.config.TARGET_UPDATE_FREQUENCY = max(100, self.config.TARGET_UPDATE_FREQUENCY - 50)
            print(f"âš¡ ì¤‘ê°„ ë‹¨ê³„ ë¶€ìŠ¤íŠ¸: íƒ€ê²Ÿ ì—…ë°ì´íŠ¸ ì£¼ê¸° â†’ {self.config.TARGET_UPDATE_FREQUENCY}")

        else:
            # ìµœì¢… ë‹¨ê³„: PER ê°•í™”
            self.agent.memory.alpha = min(1.0, self.agent.memory.alpha + 0.1)
            print(f"âš¡ ìµœì¢… ë‹¨ê³„ ë¶€ìŠ¤íŠ¸: PER Alpha â†’ {self.agent.memory.alpha:.2f}")

    def _log_advanced_metrics(self, episode: int, stats: Dict):
        """ìµœì‹  ê¸°ë²•ë“¤ì˜ ê³ ê¸‰ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        try:
            # Categorical DQN ë©”íŠ¸ë¦­
            if self.config.USE_CATEGORICAL_DQN:
                if hasattr(self.agent.q_network, 'get_categorical_distribution'):
                    # Distribution entropy (exploration ì¸¡ì •)
                    with torch.no_grad():
                        dummy_state = self._get_dummy_state()
                        dist = self.agent.q_network.get_categorical_distribution(dummy_state)
                        entropy = -(dist * torch.log(dist + 1e-8)).sum(dim=-1).mean()
                        self.categorical_metrics.append(entropy.item())

            # UCB exploration ë©”íŠ¸ë¦­
            if self.config.USE_UCB_EXPLORATION and hasattr(self.agent, 'action_counts'):
                # Action diversity
                action_probs = self.agent.action_counts / self.agent.total_actions
                action_entropy = -(action_probs * np.log(action_probs + 1e-8)).sum()
                self.ucb_metrics.append(action_entropy)

            # ë¡œê·¸ ì¶œë ¥ (ê°„ê²°í•˜ê²Œ)
            if episode % (self.config.LOG_INTERVAL * 2) == 0:
                # ê³ ê¸‰ ë©”íŠ¸ë¦­ ê¸°ë¡

                if self.categorical_metrics:
                    recent_entropy = np.mean(self.categorical_metrics[-10:])
                    print(f"  ğŸ² ë¶„í¬ ì—”íŠ¸ë¡œí”¼: {recent_entropy:.3f}")

                if self.ucb_metrics:
                    recent_diversity = np.mean(self.ucb_metrics[-10:])
                    print(f"  ğŸ¯ í–‰ë™ ë‹¤ì–‘ì„±: {recent_diversity:.3f}")

                # GPU memory tracking (silent)
                if torch.cuda.is_available():
                    memory_used = torch.cuda.memory_allocated() / 1024**3  # GB
                    memory_cached = torch.cuda.memory_reserved() / 1024**3  # GB
                    # Memory tracked silently

        except Exception as e:
            print(f"âš ï¸ ê³ ê¸‰ ë©”íŠ¸ë¦­ ë¡œê¹… ì˜¤ë¥˜: {e}")

    def _get_dummy_state(self):
        """ë”ë¯¸ ìƒíƒœ ìƒì„± (ë©”íŠ¸ë¦­ ê³„ì‚°ìš©)"""
        return {
            'image': torch.randn(1, self.config.IMAGE_CHANNELS,
                               self.config.IMAGE_HEIGHT, self.config.IMAGE_WIDTH).to(self.agent.device),
            'position': torch.randn(1, 9).to(self.agent.device)
        }

    def train(self):
        """í™•ì¥ í•™ìŠµ ë©”ì¸ ë£¨í”„"""
        if not self.initialize_components():
            print("âŒ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
            return

        print(f"Starting Extended Training!")
        print("=" * 70)

        training_start_time = time.time()

        try:
            for episode in range(self.start_episode, self.config.NUM_EPISODES):
                if self.training_interrupted:
                    break

                display_episode = episode + 1

                # ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œ ì‹œì‘ êµ¬ë¶„ì„ 
                if display_episode == 1:
                    print("\n" + "=" * 90)
                    # ì—í”¼ì†Œë“œ ì‹œì‘
                    print("=" * 90)

                # ì—í”¼ì†Œë“œ ì‹¤í–‰
                episode_stats = self.run_episode(display_episode)

                # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                perf_metrics = self.monitor.update()

                # í†µí•© í†µê³„
                combined_stats = {**episode_stats, **perf_metrics}

                # ì„±ëŠ¥ ë¶„ì„ê¸°ì— ë°ì´í„° ì¶”ê°€
                combined_stats['collision'] = episode_stats.get('reason') == 'collision'
                self.performance_analyzer.add_episode_data(display_episode, combined_stats)

                # ì§„í–‰ìƒí™© ë¡œê¹…
                if episode % self.config.LOG_INTERVAL == 0:
                    # print_curriculum_progress(episode, self.config)  # ë¹„í™œì„±í™”
                    self.logger.log_episode(display_episode, combined_stats)

                    # ì„±ëŠ¥ ìš”ì•½
                    recent_avg = np.mean(self.recent_rewards) if self.recent_rewards else 0
                    elapsed_time = time.time() - training_start_time
                    eta = calculate_eta(episode - self.start_episode,
                                      self.config.NUM_EPISODES - self.start_episode,
                                      elapsed_time)

                    print(f"\nEpisode {display_episode:4d}/{self.config.NUM_EPISODES} | "
                          f"ë³´ìƒ: {episode_stats['reward']:7.1f} | "
                          f"í‰ê· : {recent_avg:7.1f} | "
                          f"Stage: {episode_stats['stage'][:12]:<12} | "
                          f"LR: {episode_stats['learning_rate']:.6f} | "
                          f"ETA: {eta}")

                    # ìˆ˜ë ´ ë¶„ì„
                    if len(self.convergence_tracker) >= 30:
                        convergence_std = np.std(self.convergence_tracker[-20:])
                        print(f"[CONVERGENCE] ìˆ˜ë ´ ì•ˆì •ì„±: {convergence_std:.2f} | ê°€ì†ë„: {self.learning_acceleration:+.2f}")

                    # ê¶Œì¥ì‚¬í•­ ì¶œë ¥ (ë” ì ì€ ë¹ˆë„)
                    if episode % (self.config.LOG_INTERVAL * 4) == 0:
                        recommendations = get_stage_recommendations(episode, self.config)
                        print(f"[TIPS] í˜„ì¬ ë‹¨ê³„ ê¶Œì¥ì‚¬í•­: {recommendations[0]}")

                # ë§ˆì¼ìŠ¤í†¤ ì²´í¬
                self.check_milestones(display_episode, combined_stats)

                # ì²´í¬í¬ì¸íŠ¸ ì €ì¥
                if episode % self.config.SAVE_FREQUENCY == 0:
                    self.save_checkpoint(display_episode, combined_stats)

                # ì ê·¹ì ì¸ ë©”ëª¨ë¦¬ ìµœì í™” (RTX 3060Ti 8GB)
                if episode % 50 == 0:
                    MemoryManager.optimize_memory()

                # GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë” ìì£¼ ì‹¤í–‰)
                if episode % 25 == 0 and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()  # GPU ë™ê¸°í™”ë¡œ ì™„ì „í•œ ë©”ëª¨ë¦¬ ì •ë¦¬

                # ì—í”¼ì†Œë“œ í†µê³„ ì •ë¦¬ (ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
                if hasattr(self, 'recent_rewards') and len(self.recent_rewards) > 200:
                    self.recent_rewards = self.recent_rewards[-100:]  # ìµœê·¼ 100ê°œë§Œ ìœ ì§€

                # ìµœì‹  ê¸°ë²• ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
                if episode % self.config.LOG_INTERVAL == 0:
                    self._log_advanced_metrics(episode, combined_stats)

        except Exception as e:
            print(f"[ERROR] í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

        finally:
            self._cleanup(episode if 'episode' in locals() else self.start_episode)

    def save_checkpoint(self, episode: int, stats: Dict):
        """í™•ì¥ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
        # ìŠ¤í…Œì´ì§€ë³„ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
        current_stage = stats.get('stage', 'unknown')
        if current_stage not in self.stage_best_rewards:
            self.stage_best_rewards[current_stage] = float('-inf')

        if stats['reward'] > self.stage_best_rewards[current_stage]:
            self.stage_best_rewards[current_stage] = stats['reward']
            stage_best_path = os.path.join(self.config.SAVE_DIR, f"best_{current_stage}.pth")
            self.agent.save_model(stage_best_path, episode)

        # ì •ê¸° ì²´í¬í¬ì¸íŠ¸
        self.checkpointer.save_checkpoint(self.agent, episode, stats)

        # ë§ˆìŠ¤í„° ë ˆë²¨ ë‹¬ì„± ì‹œ íŠ¹ë³„ ì €ì¥
        if episode >= 950 and stats['reward'] > 250:
            master_path = os.path.join(self.config.SAVE_DIR, f"master_level_ep{episode}.pth")
            self.agent.save_model(master_path, episode)

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if episode >= self.config.NUM_EPISODES - 1:
            final_path = os.path.join(self.config.SAVE_DIR, "complete_model.pth")
            self.agent.save_model(final_path, episode)
            print(f"ì™„ì„± ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_path}")

    def _cleanup(self, final_episode: int):
        """í™•ì¥ í•™ìŠµ ì¢…ë£Œ ì •ë¦¬"""
        print("\n" + "=" * 70)
        print("í™•ì¥ í•™ìŠµ ì™„ë£Œ!")

        # ì„±ëŠ¥ ë¶„ì„ ìš”ì•½ ì¶œë ¥
        self.performance_analyzer.print_summary()

        # ìƒì„¸ ë³´ê³ ì„œ ìƒì„±
        print("\nğŸ“ˆ Generating Extended Performance Analysis Report...")
        try:
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            excel_filename = f"DQN_1000EP_Extended_Performance_{timestamp}.xlsx"
            excel_path = self.performance_analyzer.generate_excel_report(excel_filename)

            json_filename = f"DQN_1000EP_Extended_RawData_{timestamp}.json"
            json_path = self.performance_analyzer.save_raw_data(json_filename)

            print(f"âœ… í™•ì¥ Excel ë³´ê³ ì„œ: {excel_path}")
            print(f"âœ… í™•ì¥ ì›ì‹œ ë°ì´í„°: {json_path}")

        except Exception as e:
            print(f"âŒ ë³´ê³ ì„œ ìƒì„± ì‹¤íŒ¨: {e}")

        # ìŠ¤í…Œì´ì§€ë³„ ì„±ê³¼ ìš”ì•½
        print("\nğŸ“Š ìŠ¤í…Œì´ì§€ë³„ ìµœì¢… ì„±ê³¼:")
        for stage, rewards in self.stage_performance.items():
            if rewards:
                avg_reward = np.mean(rewards)
                max_reward = max(rewards)
                improvement = max_reward - rewards[0] if len(rewards) > 1 else 0
                print(f"  {stage}: í‰ê·  {avg_reward:.1f}, ìµœê³  {max_reward:.1f}, ê°œì„  {improvement:+.1f} ({len(rewards)}íšŒ)")

        # ë§ˆì¼ìŠ¤í†¤ ë‹¬ì„±ë¥ 
        achievement_rate = len(self.milestone_achieved) / len(CURRICULUM_MILESTONES) * 100
        print(f"\në§ˆì¼ìŠ¤í†¤ ë‹¬ì„±ë¥ : {achievement_rate:.1f}% ({len(self.milestone_achieved)}/{len(CURRICULUM_MILESTONES)})")

        # ìµœì¢… ì„±ëŠ¥ í‰ê°€
        final_performance = np.mean(self.recent_rewards[-50:]) if len(self.recent_rewards) >= 50 else 0
        readiness_score = min(100, max(0, (final_performance + 200) / 5))

        print(f"\nğŸ¯ ìµœì¢… ì„±ëŠ¥ ì ìˆ˜: {readiness_score:.1f}%")
        if readiness_score >= 85:
            print("âœ… ë§ˆìŠ¤í„° ë ˆë²¨ ë‹¬ì„±! ì‹¤ì „ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ!")
        elif readiness_score >= 70:
            print("âœ… ê³ ê¸‰ ìˆ˜ì¤€ ë‹¬ì„±! ì¶”ê°€ ìµœì í™” ê¶Œì¥")
        else:
            print("âš ï¸ ì¶”ê°€ í•™ìŠµì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # ìµœì¢… ëª¨ë¸ ì €ì¥
        if hasattr(self, 'agent') and self.agent:
            try:
                timestamp = time.strftime("%Y%m%d_%H%M%S")

                # ìµœì¢… ì™„ì„± ëª¨ë¸
                final_model_path = f"models/DQN_1000EP_Final_{timestamp}.pth"
                os.makedirs("models", exist_ok=True)

                model_state = {
                    'q_network_state_dict': self.agent.q_network.state_dict(),
                    'target_network_state_dict': self.agent.target_network.state_dict(),
                    'optimizer_state_dict': self.agent.optimizer.state_dict(),
                    'config': self.config.__dict__,
                    'final_episode': final_episode,
                    'training_complete': True,
                    'model_type': 'DQN_1000EP_Complete',
                    'final_performance': final_performance,
                    'readiness_score': readiness_score,
                    'milestone_achieved': list(self.milestone_achieved),
                    'save_timestamp': timestamp
                }

                torch.save(model_state, final_model_path)
                print(f"âœ… ìµœì¢… 1000EP ëª¨ë¸ ì €ì¥: {final_model_path}")

                # ë§ˆìŠ¤í„° ë ˆë²¨ ì¶”ë¡  ëª¨ë¸
                if readiness_score >= 85:
                    master_inference_path = f"models/DQN_Master_Inference_{timestamp}.pth"
                    inference_state = {
                        'q_network_state_dict': self.agent.q_network.state_dict(),
                        'config': {
                            'STATE_SIZE': self.config.STATE_SIZE,
                            'ACTION_SIZE': self.config.ACTION_SIZE,
                            'IMAGE_WIDTH': self.config.IMAGE_WIDTH,
                            'IMAGE_HEIGHT': self.config.IMAGE_HEIGHT,
                            'IMAGE_CHANNELS': self.config.IMAGE_CHANNELS,
                            'CNN_FILTERS': self.config.CNN_FILTERS,
                            'CNN_KERNELS': self.config.CNN_KERNELS,
                            'CNN_STRIDES': self.config.CNN_STRIDES,
                            'HIDDEN_SIZES': self.config.HIDDEN_SIZES,
                            'USE_DUELING_DQN': self.config.USE_DUELING_DQN,
                            'TARGET_POSITIONS': self.config.TARGET_POSITIONS,
                            'TARGET_RADIUS': self.config.TARGET_RADIUS
                        },
                        'model_type': 'DQN_Master_Inference',
                        'training_episodes': final_episode,
                        'performance_score': readiness_score,
                        'save_timestamp': timestamp
                    }

                    torch.save(inference_state, master_inference_path)
                    print(f"âœ… ë§ˆìŠ¤í„° ì¶”ë¡  ëª¨ë¸ ì €ì¥: {master_inference_path}")

            except Exception as e:
                print(f"âŒ ìµœì¢… ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")

        # ì‹œê°í™” ìƒì„±
        if hasattr(self, 'logger') and self.logger:
            try:
                self.visualizer.plot_training_curves(self.logger.episode_data, "extended_1000_training")
                print("âœ… í™•ì¥ í•™ìŠµ ê³¡ì„  ì €ì¥ ì™„ë£Œ")
            except Exception as e:
                print(f"âš ï¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")

        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„° ì •ë¦¬
        if hasattr(self, 'real_time_monitor'):
            self.real_time_monitor.cleanup_terminal()

        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
        if hasattr(self, 'env') and self.env:
            self.env.close()
        if hasattr(self, 'logger') and self.logger:
            self.logger.close()

        print("í™•ì¥ í•™ìŠµ ì™„ë£Œ - ë§ˆìŠ¤í„° ë ˆë²¨ ë“œë¡  AI ì™„ì„±!")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='í™•ì¥ DQN í•™ìŠµ')
    parser.add_argument('--experiment', type=str, default=None, help='ì‹¤í—˜ ì´ë¦„')
    parser.add_argument('--resume', type=str, default=None, help='ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘')
    parser.add_argument('--no-realtime', action='store_true', help='ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™”')

    args = parser.parse_args()

    print("DQN AirSim Extended Training")
    print("Advanced Deep Reinforcement Learning for Master-Level Autonomous Flight")
    print("=" * 70)

    # ì»¤ë¦¬í˜ëŸ¼ ìš”ì•½
    config = CurriculumConfig()
    print("5ë‹¨ê³„ í™•ì¥ ì»¤ë¦¬í˜ëŸ¼:")
    for stage_name, stage_config in config.CURRICULUM_STAGES.items():
        episodes = stage_config['episodes']
        focus = stage_config['focus']
        steps = stage_config['max_episode_steps']
        print(f"  {episodes[0]:3d}-{episodes[1]:3d}: {focus} (Max {steps} steps)")

    print(f"\n{len(CURRICULUM_MILESTONES)}ê°œ ë§ˆì¼ìŠ¤í†¤ìœ¼ë¡œ ì™„ì „í•œ ë§ˆìŠ¤í„° ë ˆë²¨ ë‹¬ì„±!")
    print("=" * 70)

    # í•™ìŠµ ì‹œì‘
    trainer = ExtendedTrainingManager(args.experiment, args.resume)

    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
    if args.no_realtime:
        trainer.use_real_time_display = False
        print("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë¹„í™œì„±í™” - ê¸°ë³¸ ë¡œê·¸ ì¶œë ¥")
    else:
        print("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ í™œì„±í™” - ê³ ê¸‰ ë™ì  ì½˜ì†” ì¶œë ¥")
        print("í•™ìŠµ ì¤‘ Ctrl+Cë¡œ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ ê°€ëŠ¥")
        time.sleep(2)

    trainer.train()

if __name__ == "__main__":
    main()