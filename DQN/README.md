# ğŸš DQN AirSim ë¬´ì¸ ì¸ëª… íƒìƒ‰ ë“œë¡ 

AirSim ì‹œë®¬ë ˆì´í„° ê¸°ë°˜ì˜ Deep Q-Network(DQN) ê°•í™”í•™ìŠµì„ í™œìš©í•œ ììœ¨ ì¸ëª… íƒìƒ‰ ë“œë¡  í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” Unreal Engine ê¸°ë°˜ AirSim ì‹œë®¬ë ˆì´í„°ì—ì„œ DQN ê°•í™”í•™ìŠµì„ í†µí•´ ë“œë¡ ì´ ììœ¨ì ìœ¼ë¡œ ì¸ëª…ì„ íƒìƒ‰í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

### ğŸ¯ ì£¼ìš” íŠ¹ì§•

- **ìµœì‹  DQN ê¸°ë²• ì ìš©**: Double DQN, Dueling DQN, Prioritized Experience Replay
- **Multi-step Learning**: 3-step ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ìœ¼ë¡œ í•™ìŠµ íš¨ìœ¨ì„± í–¥ìƒ
- **Mixed Precision Training**: RTX 3060Ti ë©”ëª¨ë¦¬ ìµœì í™”
- **ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§**: TensorBoard, ì„±ëŠ¥ ë©”íŠ¸ë¦­, ì‹œê°í™”
- **ê²¬ê³ í•œ í™˜ê²½ ì„¤ê³„**: ì¶©ëŒ ê°ì§€, ê²½ê³„ ì²´í¬, ë³´ìƒ í•¨ìˆ˜ ìµœì í™”

## ğŸ› ï¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•˜ë“œì›¨ì–´
- **CPU**: AMD Ryzen 5 5600X (6ì½”ì–´) ë˜ëŠ” ë™ê¸‰
- **RAM**: 16GB DDR4-3200 ì´ìƒ
- **GPU**: RTX 3060Ti (8GB VRAM) ë˜ëŠ” ë™ê¸‰ ì´ìƒ
- **ì €ì¥ê³µê°„**: 10GB ì´ìƒ

### ì†Œí”„íŠ¸ì›¨ì–´
- **OS**: Windows 10/11 (AirSim ê¶Œì¥)
- **Python**: 3.10
- **AirSim**: 1.8.1
- **Unreal Engine**: 4.27 (AirSim í˜¸í™˜ ë²„ì „)

## ğŸš€ ì„¤ì¹˜ ë° ì„¤ì •

### 1. AirSim ì„¤ì¹˜

```bash
# AirSim ì„¤ì¹˜ (Windows)
# 1. Unreal Engine 4.27 ì„¤ì¹˜
# 2. AirSim ë¹Œë“œ ë° ì„¤ì¹˜
# ìì„¸í•œ ì„¤ì¹˜ ë°©ë²•: https://microsoft.github.io/AirSim/
```

### 2. Python í™˜ê²½ ì„¤ì •

```bash
# ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
python -m venv venv
venv\\Scripts\\activate  # Windows

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. AirSim ì„¤ì •

`Documents/AirSim/settings.json` íŒŒì¼ ìƒì„±:

```json
{
  "SeeDocsAt": "https://github.com/Microsoft/AirSim/blob/main/docs/settings.md",
  "SettingsVersion": 1.2,
  "SimMode": "Multirotor",
  "Vehicles": {
    "Drone1": {
      "VehicleType": "SimpleFlight",
      "DefaultVehicleState": "Armed",
      "EnableCollisionPassthroughOnMove": false,
      "EnableCollisions": true
    }
  },
  "CameraDefaults": {
    "CaptureSettings": [
      {
        "ImageType": 0,
        "Width": 320,
        "Height": 240,
        "FOV_Degrees": 90
      }
    ]
  },
  "ClockSpeed": 1.0
}
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
DQN_No_Human/
â”œâ”€â”€ config.py              # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì„¤ì •
â”œâ”€â”€ environment.py          # AirSim í™˜ê²½ ë˜í¼
â”œâ”€â”€ network.py             # DQN ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…ì²˜
â”œâ”€â”€ dqn_agent.py           # DQN ì—ì´ì „íŠ¸ í´ë˜ìŠ¤
â”œâ”€â”€ utils.py               # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”œâ”€â”€ train.py               # ë©”ì¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ requirements.txt       # í•„ìš” íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ README.md             # í”„ë¡œì íŠ¸ ë¬¸ì„œ
â””â”€â”€ experiments/          # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ì†Œ
    â”œâ”€â”€ logs/            # í•™ìŠµ ë¡œê·¸
    â”œâ”€â”€ checkpoints/     # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
    â””â”€â”€ plots/           # ì‹œê°í™” ê²°ê³¼
```

## ğŸ® ì‚¬ìš©ë²•

### í•™ìŠµ ì‹œì‘

```bash
# ê¸°ë³¸ í•™ìŠµ
python train.py

# ì‹¤í—˜ ì´ë¦„ ì§€ì •
python train.py --experiment my_experiment

# ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
python train.py --resume checkpoints/latest_model.pth

# í‰ê°€ ì „ìš© ëª¨ë“œ
python train.py --eval_only --resume checkpoints/best_model.pth
```

### ì„¤ì • ìˆ˜ì •

`config.py` íŒŒì¼ì—ì„œ ì£¼ìš” ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°
LEARNING_RATE = 0.0001      # í•™ìŠµë¥ 
BATCH_SIZE = 32             # ë°°ì¹˜ í¬ê¸°
EPSILON_DECAY = 0.995       # íƒí—˜ë¥  ê°ì†Œ
NUM_EPISODES = 2000         # ì´ ì—í”¼ì†Œë“œ ìˆ˜

# ë³´ìƒ í•¨ìˆ˜ ì¡°ì •
REWARDS = {
    'target_reached': 100.0,     # ëª©í‘œ ë„ë‹¬ ë³´ìƒ
    'collision': -100.0,         # ì¶©ëŒ í˜ë„í‹°
    'exploration_bonus': 0.2,    # íƒìƒ‰ ë³´ë„ˆìŠ¤
}
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§

### TensorBoard ì‹¤í–‰

```bash
tensorboard --logdir experiments/logs/tensorboard
# http://localhost:6006 ì—ì„œ í™•ì¸
```

### ì£¼ìš” ë©”íŠ¸ë¦­

- **Episode Reward**: ì—í”¼ì†Œë“œë³„ ëˆ„ì  ë³´ìƒ
- **Targets Found**: ë°œê²¬í•œ ëª©í‘œ ì¸ëª… ìˆ˜
- **Exploration Coverage**: íƒìƒ‰í•œ ì˜ì—­ ë¹„ìœ¨
- **Training Loss**: í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜
- **Q-Value Distribution**: Q ê°’ ë¶„í¬

## ğŸ§  DQN ì•„í‚¤í…ì²˜

### ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°

1. **CNN Backbone**: ë“œë¡  ì¹´ë©”ë¼ ì´ë¯¸ì§€ ì²˜ë¦¬
   - 3ê°œ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´ (32, 64, 64 í•„í„°)
   - BatchNorm + ReLU í™œì„±í™”
   - Adaptive Average Pooling

2. **Position Encoder**: ë“œë¡  ìœ„ì¹˜/ìì„¸ ì •ë³´ ì²˜ë¦¬
   - 9ì°¨ì› ì…ë ¥ (x,y,z,roll,pitch,yaw,target_rel_x,y,z)
   - 2ì¸µ MLP (128 â†’ 256)

3. **Feature Fusion**: ì´ë¯¸ì§€ì™€ ìœ„ì¹˜ ì •ë³´ ê²°í•©
   - Concatenation + MLP (512 â†’ 256)
   - Self-Attention ë©”ì»¤ë‹ˆì¦˜

4. **Dueling Head**: ê°€ì¹˜ í•¨ìˆ˜ ë¶„ë¦¬
   - Value Stream: ìƒíƒœ ê°€ì¹˜ V(s)
   - Advantage Stream: í–‰ë™ ìš°ìœ„ A(s,a)
   - Q(s,a) = V(s) + A(s,a) - mean(A(s,a))

### ìµœì‹  ê¸°ë²• ì ìš©

- **Double DQN**: ê³¼ëŒ€ì¶”ì • ë°©ì§€
- **Prioritized Experience Replay**: ì¤‘ìš”í•œ ê²½í—˜ ìš°ì„  í•™ìŠµ
- **Multi-step Learning**: 3-step ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘
- **Mixed Precision**: GPU ë©”ëª¨ë¦¬ ìµœì í™”
- **Gradient Clipping**: í•™ìŠµ ì•ˆì •ì„± í–¥ìƒ

## ğŸ¯ ë³´ìƒ í•¨ìˆ˜

```python
ë³´ìƒ = ëª©í‘œë„ë‹¬(+100) + ì ‘ê·¼ë³´ìƒ(+1~+5) + íƒìƒ‰ë³´ë„ˆìŠ¤(+0.2)
     - ì¶©ëŒí˜ë„í‹°(-100) - ê²½ê³„ì´íƒˆ(-50) - ì‹œê°„í˜ë„í‹°(-0.1)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

### GPU ë©”ëª¨ë¦¬ ìµœì í™”
- ì´ë¯¸ì§€ í¬ê¸°: 640x480 â†’ 320x240
- ë°°ì¹˜ í¬ê¸°: 32 (RTX 3060Ti ìµœì í™”)
- Mixed Precision Training
- Gradient Accumulation

### CPU ë³‘ë ¬ ì²˜ë¦¬
- 4ê°œ ì›Œì»¤ ìŠ¤ë ˆë“œ í™œìš©
- ë¹„ë™ê¸° ë°ì´í„° ìˆ˜ì§‘
- ê²½í—˜ ì¬ìƒ ë²„í¼ ìµœì í™”

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œë“¤

1. **AirSim ì—°ê²° ì‹¤íŒ¨**
   ```bash
   # AirSimì´ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸
   # settings.json ì„¤ì • í™•ì¸
   # ë°©í™”ë²½ ì„¤ì • í™•ì¸
   ```

2. **GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**
   ```python
   # config.pyì—ì„œ ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸°
   BATCH_SIZE = 16  # ê¸°ë³¸ê°’ 32ì—ì„œ ê°ì†Œ
   ```

3. **í•™ìŠµ ë¶ˆì•ˆì •**
   ```python
   # í•™ìŠµë¥  ë‚®ì¶”ê¸°
   LEARNING_RATE = 0.00005
   # Replay buffer í¬ê¸° ëŠ˜ë¦¬ê¸°
   REPLAY_BUFFER_SIZE = 100000
   ```

## ğŸ“ ì‹¤í—˜ ê°€ì´ë“œ

### í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìˆœì„œ

1. **í•™ìŠµë¥ **: 0.0001 â†’ 0.00005 â†’ 0.0002
2. **ë°°ì¹˜ í¬ê¸°**: 32 â†’ 64 â†’ 16
3. **ë„¤íŠ¸ì›Œí¬ í¬ê¸°**: í•„í„° ìˆ˜, ì€ë‹‰ì¸µ í¬ê¸° ì¡°ì •
4. **ë³´ìƒ í•¨ìˆ˜**: ê° ìš”ì†Œë³„ ê°€ì¤‘ì¹˜ ì¡°ì •

### ì„±ëŠ¥ ì§€í‘œ

- **ìˆ˜ë ´ ì†ë„**: 1000 ì—í”¼ì†Œë“œ ë‚´ ì•ˆì •í™” ëª©í‘œ
- **íƒìƒ‰ íš¨ìœ¨**: 90% ì´ìƒ ì˜ì—­ ì»¤ë²„ë¦¬ì§€
- **ëª©í‘œ ë‹¬ì„±ë¥ **: 80% ì´ìƒ ì„±ê³µë¥ 

## ğŸ¤ ê¸°ì—¬ ë°©ë²•

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„¼ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ™ ê°ì‚¬ì˜ ë§

- [Microsoft AirSim](https://github.com/microsoft/AirSim) - ì‹œë®¬ë ˆì´ì…˜ í™˜ê²½
- [PyTorch](https://pytorch.org/) - ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- [OpenAI Gym](https://gym.openai.com/) - ê°•í™”í•™ìŠµ ì¸í„°í˜ì´ìŠ¤

## ğŸ“š ì°¸ê³  ë¬¸í—Œ

1. Mnih, V., et al. "Human-level control through deep reinforcement learning." Nature (2015)
2. Van Hasselt, H., et al. "Deep reinforcement learning with double q-learning." AAAI (2016)
3. Wang, Z., et al. "Dueling network architectures for deep reinforcement learning." ICML (2016)
4. Schaul, T., et al. "Prioritized experience replay." ICLR (2016)
5. Fortunato, M., et al. "Noisy networks for exploration." ICLR (2018)

---

**ğŸ“§ ë¬¸ì˜ì‚¬í•­**: í”„ë¡œì íŠ¸ ê´€ë ¨ ë¬¸ì˜ì‚¬í•­ì€ Issues íƒ­ì„ ì´ìš©í•´ ì£¼ì„¸ìš”.

**â­ Star**: ì´ í”„ë¡œì íŠ¸ê°€ ë„ì›€ì´ ë˜ì—ˆë‹¤ë©´ Starë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”!